name: Karachi AQI CI/CD Pipeline

# Automated CI/CD pipeline for Karachi AQI prediction system
# - Data collection: Every hour
# - Model training: Daily at 2 AM UTC
# - Manual trigger: Available via workflow_dispatch

on:
  schedule:
    # Run data collection every hour
    - cron: '0 * * * *'
    # Run model training daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch: # Allow manual triggering
  push:
    branches: [ main, master ]
    paths:
      - 'src/**'
      - 'karachi_aqi_app.py'
      - 'train_karachi_models.py'

jobs:
  data-collection:
    if: github.event.schedule == '0 * * * *' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run data collection
      env:
        OPENWEATHER_API_KEY: ${{ secrets.OPENWEATHER_API_KEY }}
      run: |
        python -c "
        import sys
        sys.path.append('.')
        from karachi_aqi_app import KarachiAQISystem
        system = KarachiAQISystem()
        weather = system.get_current_weather()
        if weather:
            system.store_current_data(weather)
            print(f'‚úÖ Collected and stored weather data: AQI {weather.get(\"aqi\", \"N/A\")}')
        else:
            print('‚ùå Failed to collect weather data')
            sys.exit(1)
        "

    - name: Data quality check
      run: |
        python -c "
        import pandas as pd
        import os
        from datetime import datetime, timedelta

        data_file = 'data/karachi_raw_data.csv'
        if os.path.exists(data_file):
            df = pd.read_csv(data_file)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            recent_data = df[df['timestamp'] >= (datetime.now() - timedelta(hours=1))]

            print(f'üìä Data quality check:')
            print(f'   Total records: {len(df)}')
            print(f'   Recent records (last hour): {len(recent_data)}')
            print(f'   Missing values: {df.isnull().sum().sum()}')

            if len(recent_data) == 0:
                print('‚ö†Ô∏è  Warning: No data collected in last hour')
        else:
            print('‚ùå Data file not found')
            exit(1)
        "

  model-training:
    if: github.event.schedule == '0 2 * * *' || (github.event_name == 'workflow_dispatch' && github.event.inputs.train_models == 'true')
    runs-on: ubuntu-latest
    needs: data-collection

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Check if training is needed
      id: check_training
      run: |
        python -c "
        import os
        from pathlib import Path
        import time

        models_dir = Path('src/models/saved_models')
        needs_training = 'true'

        if models_dir.exists():
            current_time = time.time()
            recent_models = False

            for model_file in models_dir.glob('*karachi*.joblib'):
                if (current_time - model_file.stat().st_mtime) < (24 * 3600):  # 24 hours
                    recent_models = True
                    break

            if recent_models:
                needs_training = 'false'

        print(f'needs_training={needs_training}')
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f'needs_training={needs_training}\\n')
        "

    - name: Run model training
      if: steps.check_training.outputs.needs_training == 'true'
      env:
        OPENWEATHER_API_KEY: ${{ secrets.OPENWEATHER_API_KEY }}
      run: |
        python train_karachi_models.py

    - name: Evaluate model performance
      if: steps.check_training.outputs.needs_training == 'true'
      run: |
        python -c "
        import os
        import json
        from pathlib import Path

        models_dir = Path('src/models/saved_models')
        perf_files = list(models_dir.glob('performance*karachi*.json'))

        if perf_files:
            latest_perf = max(perf_files, key=lambda x: x.stat().st_mtime)

            with open(latest_perf, 'r') as f:
                performance = json.load(f)

            print('ü§ñ Model Performance Results:')
            for model_name, metrics in performance.items():
                if isinstance(metrics, dict) and 'test_r2' in metrics:
                    r2_score = metrics['test_r2']
                    mae = metrics.get('test_mae', 'N/A')
                    print(f'   {model_name}: R¬≤ = {r2_score:.3f}, MAE = {mae}')

                    # Determine best model
                    if model_name == 'random_forest':
                        print(f'   üèÜ Best model: {model_name} (R¬≤ = {r2_score:.3f})')
        else:
            print('‚ùå No performance files found')
        "

    - name: Upload model artifacts
      if: steps.check_training.outputs.needs_training == 'true'
      uses: actions/upload-artifact@v3
      with:
        name: trained-models
        path: |
          src/models/saved_models/*karachi*.joblib
          src/models/saved_models/*karachi*.json
        retention-days: 30

  deploy:
    if: github.event.schedule == '0 2 * * *' && needs.model-training.result == 'success'
    runs-on: ubuntu-latest
    needs: model-training

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Deploy notification
      run: |
        echo "üéâ Karachi AQI Pipeline completed successfully!"
        echo "‚úÖ Data collection: Completed"
        echo "‚úÖ Model training: Completed"
        echo "‚úÖ Model evaluation: Completed"
        echo "üöÄ Models ready for production use"
